{"componentChunkName":"component---src-pages-index-js","path":"/","result":{"data":{"hero":{"edges":[{"node":{"frontmatter":{"title":"Hello World! My name is","name":"Dr. Avinash Kumar Singh","subtitle":"I love creating AI applications!","buttonText":"Contact Me"},"html":"<p>I am a researcher specialized in Computer Vision and Human Robot Interaction. Currently serving as Director AI in AI Labs at <a href=\"https://www.braneenterprises.com/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Brane Enterprises Pvt Ltd</a>. Outside work, I used to spread awareness about the artificial intelligence and help underprivileged students to get jobs in this sector through our <a href=\"https://www.robaita.com/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Robaita initiative</a>.</p>"}}]},"about":{"edges":[{"node":{"frontmatter":{"title":"About Me","avatar":{"childImageSharp":{"fluid":{"tracedSVG":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAANABQDASIAAhEBAxEB/8QAFwAAAwEAAAAAAAAAAAAAAAAAAAIEBf/EABUBAQEAAAAAAAAAAAAAAAAAAAEC/9oADAMBAAIQAxAAAAFnoYnGKyX/xAAZEAEBAAMBAAAAAAAAAAAAAAABAgMREgD/2gAIAQEAAQUC4uqHnG7r0mhkC8Z1/8QAFxEBAAMAAAAAAAAAAAAAAAAAAAESYf/aAAgBAwEBPwHFZf/EABcRAAMBAAAAAAAAAAAAAAAAAAARIRL/2gAIAQIBAT8BzGQ//8QAGhAAAgIDAAAAAAAAAAAAAAAAAAECIRExQf/aAAgBAQAGPwLQ5JUZKGuMo//EABoQAQADAQEBAAAAAAAAAAAAAAEAESExUWH/2gAIAQEAAT8hozr6zkxad2yWui/GDd1hcpZoqQFEh5P/2gAMAwEAAgADAAAAEHwv/8QAFxEBAAMAAAAAAAAAAAAAAAAAARARIf/aAAgBAwEBPxBdtB//xAAWEQEBAQAAAAAAAAAAAAAAAAABABH/2gAIAQIBAT8QAxs//8QAGxABAAMBAQEBAAAAAAAAAAAAAQARMSFhQVH/2gAIAQEAAT8QrmwXvpsFtWEcGrH2dS7qoh8AXJtX+wIBZ+KpcQARQPk//9k=","aspectRatio":1.4957264957264957,"src":"/static/8e5345a542fe8765fd41a744d519551f/ea4ab/me.jpg","srcSet":"/static/8e5345a542fe8765fd41a744d519551f/477ba/me.jpg 175w,\n/static/8e5345a542fe8765fd41a744d519551f/06776/me.jpg 350w,\n/static/8e5345a542fe8765fd41a744d519551f/ea4ab/me.jpg 700w,\n/static/8e5345a542fe8765fd41a744d519551f/3055e/me.jpg 1050w,\n/static/8e5345a542fe8765fd41a744d519551f/eff08/me.jpg 1400w,\n/static/8e5345a542fe8765fd41a744d519551f/ac71e/me.jpg 7952w","srcWebp":"/static/8e5345a542fe8765fd41a744d519551f/89afa/me.webp","srcSetWebp":"/static/8e5345a542fe8765fd41a744d519551f/9fca7/me.webp 175w,\n/static/8e5345a542fe8765fd41a744d519551f/37a4e/me.webp 350w,\n/static/8e5345a542fe8765fd41a744d519551f/89afa/me.webp 700w,\n/static/8e5345a542fe8765fd41a744d519551f/78e7a/me.webp 1050w,\n/static/8e5345a542fe8765fd41a744d519551f/03d34/me.webp 1400w,\n/static/8e5345a542fe8765fd41a744d519551f/8d03c/me.webp 7952w","sizes":"(max-width: 700px) 100vw, 700px"}}},"skills":["Generative AI","Deep Learning","Machine Learning","Computer Vision","Natural Language Processing","Human Robot Interaction"]},"html":"<p>Hi, I am Avinash, thank you for visiting my page. I love solving real life problems with the help of Artificial Inteligence. I did my Ph.D in Human-Robot Interaction from <a href=\"https://www.iiita.ac.in/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Indian Institute of Information Technology, Allahabad</a>. During my Ph.D., we aimed to build a reliable and intelligent framework for robots, so that they could interact with human as we do in our daily life. If you are interested, please see my contributions at <a href=\"https://scholar.google.co.in/citations?user=eH9aB9kAAAAJ&#x26;hl=en\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Google Scholar</a> and my <a href=\"https://www.youtube.com/channel/UCRdJsCCYkkSp9qhqW4Wbh2Q\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Youtube page</a>.</p>\n<p>I got the opportunity to work on the extension of my Ph.D work in two subsequent post doctorals. I was a Post-Doctoral Research Fellow in Department of Computing Science, <a href=\"https://www.umu.se/en/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Umea University Sweden</a>. There, I worked on training the Pepper Humanoid Robots to assist senior citizens. The robot was programmed to understand human commands (in english) and corelate the language to visual perception. This work was conducted under the research grant given by <a href=\"https://www.kempe.com/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Kempe Fundation</a>.</p>\n<p>I was associated with LIRMM lab <a href=\"https://www.lirmm.fr/lirmm-en/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Montpellier University France</a> as a post-doctoral researcher. This was my second postdoctoral, there we trained a deep neural network model to understand human activities in industrial setup for better Human robot collaboration. I was the part of <a href=\"https://project-sophia.eu/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">H2020 project SOPHIA</a> and incharge of work package 5.</p>\n<p>I have 10+ years experience working with artificial intelligence and applying this as a tool to solve different problems. I have worked with different organization (<a href=\"https://eclerx.com/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">eClerx</a>, <a href=\"https://www.hcltech.com/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">HCL</a>, <a href=\"https://in-d.ai/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">In-D</a>) and helped them to designe, develope and execute AI based solutions.</p>\n<p>Here is a list of the technologies that I'm familiar with!</p>"}}]},"jobs":{"edges":[{"node":{"frontmatter":{"title":"Senior Solution Leader","company":"Brane","range":"May 2020 - Present","url":"https://www.braneenterprises.com/ai-product"},"html":"<p>Brane is a private Indian startup that deals in AI and Robotics product. I am currently heading the AI division, below are some of the projects that we successfully delivered.</p>\n<ul>\n<li>Implemented a Face Recognition-based office attendance system, replacing the existing RFID system, and achieving organization-wide deployment.</li>\n<li>The system serves 2,856 employees with a 97.63% accuracy rate, resulting in annual savings in operational costs.</li>\n<li>Led a groundbreaking project to design smart glasses for visually impaired individuals, providing comprehensive assistance in reading, navigation, currency identification, person recognition, and scene understanding.</li>\n<li>The system can detect obstacle up to 5 feet, can help in reading English and six Indian languages, could recognize 9,605 objects and labels with 89.76% accuracy.</li>\n<li>Successfully delivered a Driver Monitoring System (DMS), incorporating real-time monitoring and safety features.</li>\n<li>The system tracks driver drowsiness, smoking, drinking, eating, phone usage, and seatbelt compliance, resulting in a 40% reduction in driving violations.</li>\n<li>Captured and recorded over 1,000 violations with images, date, time, and other details.</li>\n<li>Provided live streams from both interior and exterior dash cameras, enhancing monitoring accuracy.</li>\n</ul>"}},{"node":{"frontmatter":{"title":"Postdoctoral Researcher","company":"Montpellier University","range":"Nov 2020 - Nov 2021","url":"https://www.lirmm.fr/lirmm-en/"},"html":"<p>I was associated with the robotics lab (LIRMM) and worked on the EU Project <a href=\"https://project-sophia.eu/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">SOPHIA</a>. As the in-charge of Work package 5, I helped my team to coordinate between different project partners <a href=\"https://www.iit.it/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Italian Institute of Technology, Italy</a>, <a href=\"https://www.inail.it/cs/internet/home.html\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">INAIL, Italy</a>, and <a href=\"https://www.lirmm.fr/lirmm-en/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">LIRMM, France</a> for data acquisition, human-robot interaction and to derive a deep learning model for action recognition.</p>\n<ul>\n<li>Designed and developed a sensor agnostic, Bidirectional LSTM based deep neural network for action recognition. The model is tested in the presence of <a href=\"https://www.xsens.com/motion-capture\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Xsens suit</a> (used for motion capture) and Intel RealSense and Microsoft Kinect RGB-D data (3D skeleton). The research is published in <a href=\"https://ieeexplore.ieee.org/abstract/document/10000226\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">21st International Conference on Humanoid Robots</a>.</li>\n<li>The model is integrated with KUKA robot to help human in physical assistance, e.g. carrying object, release object, place object etc. in industrial environment by understanding the human actions.</li>\n</ul>"}},{"node":{"frontmatter":{"title":"Lead Consultant","company":"Intain","range":"Jan 2018 - Apr 2020","url":"https://in-d.ai/"},"html":"<p>Intain is an Indian startup deals in artificial intelligence and blockchain based products. As a lead consultant my responsibilities were.</p>\n<ul>\n<li>\n<p>To lead the AI team, allocating and tracking work, and ensuring client deliveries.</p>\n</li>\n<li>\n<p>To create the pipeline of the machine learning solutions and to add new people into the team.</p>\n</li>\n</ul>"}},{"node":{"frontmatter":{"title":"Deputy Manager","company":"HCL","range":"Feb 2017 - Jan 2018","url":"https://hcltech.com/"},"html":"<p>I joined the HCL machine learning division (Noida) when this was a 3 members team. In the span of one year, we conducted 4 successful POCs and grew the team to a 16 members team.</p>\n<ul>\n<li>As a deputy manager my responsibilities were to handle the client interaction, project scoping, find the place where machine learning solutions can be pitched (to integrated with existing workflows) within and outside the organization.</li>\n<li>Helped my team to setup the machine learning GPU infrastructure, sketching project roadmaps, resource allocation and tracking. Further motivating team to follow the software engineering best practices such as maintaining git, hygiene of code (following the coding standards), etc.</li>\n</ul>"}},{"node":{"frontmatter":{"title":"Postdoctoral Researcher","company":"Umea University","range":"Feb 2018 - Jan 2020","url":"https://www.umu.se/en/"},"html":"<p>During the postdoc, I worked on humanoid robots. We trained them to assist senior citizen in the household scenarios.</p>\n<ul>\n<li>During this postdoc, I closely worked with Professor <a href=\"https://www.umu.se/en/staff/kai-florian-richter/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Kai-Florian Richter</a> and Professor <a href=\"https://www.umu.se/en/staff/thomas-hellstrom/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Thomas Hellström</a>. I was a part of intelligent robotics lab, during this postdoc, we designed a dialogue based human robot interaction system that allows humans, to talk to the robot. This work was published in a ‘A’ rated conference <a href=\"https://ecai2020.eu/papers/1441_paper.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">ECAI-2020</a>.</li>\n<li>We developed and implemented a robot collaboration framework that enables robots to have dialogues by translating their actions into the natural language. This work was published in <a href=\"https://www.degruyter.com/document/doi/10.1515/pjbr-2021-0001/html\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Journal of behavioural robotics</a> and also featured in Softbank robotics under the <a href=\"https://youtu.be/QNfaUycVAXo?t=78\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">best 20 projects in 2020</a>.</li>\n</ul>"}},{"node":{"frontmatter":{"title":"Associate Process Manager","company":"eClerx","range":"Nov 2015 - Feb 2017","url":"https://eclerx.com/"},"html":"<p>eClerx is an Indian IT consulting and outsourcing multinational company. I worked there as a full stack developer and my job was.</p>\n<ul>\n<li>To design and develop NLP, ML solutions. We used image pre-processing to improve OCR accuracy, further to integrate these solutions with RPA system.</li>\n</ul>"}}]},"featured":{"edges":[{"node":{"frontmatter":{"title":"MLOps End to End Pipeline","cover":{"childImageSharp":{"fluid":{"tracedSVG":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAAA7DAAAOwwHHb6hkAAACL0lEQVQoz43OzUuTARzA8XUIT71AGHSxiIIQrZwkExpJ2nohp4aKDjVwYviShwisIIU6dPDQpYunLnUUoj+g6MUXWNueNXPTbTr3+rxuz+P2pL3xDScSXaLDhx+/y/f3s0g+AdHnR/T6kX0CWiCA6heQvT4Unx9VEFADnzEikf9ikcQEkpgimVglGg2RTa+hySk0OY0qJlClJDk5ja5m/5JXMzuUTGk3NBFdE7Fk8lkkXWE1HSewHCSaWiOblxF1BaWYQynkUIt5NNNAM3VUUy/N3NeNkvxmAbWoI+kqsqFh+RTP8S4kM7u2wUJyi7l1k9l1k/cxg1cLMV574nxcSuAJx/EsJ/BGknhXEswvRpkLRvgghPFGU0Q1kxWlgCWQ1VmUTYJykaC0rcCStoknlqG5rRtHSweO1nb6bo3Q3nUTl/s2fe4hxu7eZ3hsnNY2F9MvZ4gVfrIobWAJigZh1SSkFgmpJktKkeXcFkI8Q925eqzVVVy5fA3H9Rs02i9QZ6vHXltHV7MTZ1snNvtVpl/MsFr8xRe5sBPcDZXIBVb0b8yH45ypqqGm2obVamOox83E6B1GO90MWBvorDxNY0MjBw9VMDn1jMQWfz7cDW7P7SsR4wdvhRBHTjSxt7yJsn1Hab/Uw/MHj5kcvYeropb+rgGand0cP1bJ+KOpfwejxnfeCGH2l59lzwE7ZYcdVJx0MuLsZaJ3mKZTDVxs6cfZMch5h4uHT56yvrkT/A2IsjPjr35iiwAAAABJRU5ErkJggg==","aspectRatio":2.108433734939759,"src":"/static/425e13010007726e809c722b2fd00a6b/60290/system_flow.png","srcSet":"/static/425e13010007726e809c722b2fd00a6b/847ef/system_flow.png 175w,\n/static/425e13010007726e809c722b2fd00a6b/91cba/system_flow.png 350w,\n/static/425e13010007726e809c722b2fd00a6b/60290/system_flow.png 700w,\n/static/425e13010007726e809c722b2fd00a6b/f5f50/system_flow.png 1050w,\n/static/425e13010007726e809c722b2fd00a6b/38674/system_flow.png 1400w,\n/static/425e13010007726e809c722b2fd00a6b/ef0be/system_flow.png 3795w","srcWebp":"/static/425e13010007726e809c722b2fd00a6b/89afa/system_flow.webp","srcSetWebp":"/static/425e13010007726e809c722b2fd00a6b/9fca7/system_flow.webp 175w,\n/static/425e13010007726e809c722b2fd00a6b/37a4e/system_flow.webp 350w,\n/static/425e13010007726e809c722b2fd00a6b/89afa/system_flow.webp 700w,\n/static/425e13010007726e809c722b2fd00a6b/78e7a/system_flow.webp 1050w,\n/static/425e13010007726e809c722b2fd00a6b/03d34/system_flow.webp 1400w,\n/static/425e13010007726e809c722b2fd00a6b/6a962/system_flow.webp 3795w","sizes":"(max-width: 700px) 100vw, 700px"}}},"tech":["OpenCV","TensorFlow","Keras","Python","Html/CSS/JS"],"github":"","external":"/blog_posts/mlops"},"html":"<p>We setup end to end flow for object detection that includes data loading, data visualization, augmentation, training, validation and model deployement on the edge device as well as on the server. We gave support for the different augmentation techniques to increase the sample count as well as flexibility to choose the model type between, Yolo, Faster-RCNN and MobileNet. At the end user can select model and the user id to push the model to be downloaded into the edge device.</p>"}},{"node":{"frontmatter":{"title":"Multi Label Image Classification","cover":{"childImageSharp":{"fluid":{"tracedSVG":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAANABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAUDBP/EABYBAQEBAAAAAAAAAAAAAAAAAAEAAv/aAAwDAQACEAMQAAAB6mU8qSaw/wD/xAAYEAADAQEAAAAAAAAAAAAAAAAAAQMCEv/aAAgBAQABBQLLRqmBVmUXEcjP/8QAFhEBAQEAAAAAAAAAAAAAAAAAAQAR/9oACAEDAQE/AQsL/8QAFhEBAQEAAAAAAAAAAAAAAAAAAAIS/9oACAECAQE/Aapp/8QAGhAAAgIDAAAAAAAAAAAAAAAAABEBQRAxMv/aAAgBAQAGPwLo2WO5Hj//xAAaEAEAAgMBAAAAAAAAAAAAAAABABEhQVFx/9oACAEBAAE/IRkD3fIApg97byOnqZN5n//aAAwDAQACAAMAAAAQmy//xAAWEQEBAQAAAAAAAAAAAAAAAAAhAAH/2gAIAQMBAT8QBswL/8QAFxEBAQEBAAAAAAAAAAAAAAAAAQAhUf/aAAgBAgEBPxBTSH2//8QAHRABAAICAgMAAAAAAAAAAAAAAQARITFBUWFx8P/aAAgBAQABPxAMC9uJaU3wwQDkG8EpBr1bQ3glCQ4HVfMQqovTRP/Z","aspectRatio":1.5086206896551724,"src":"/static/6fb008f2b044681631d2196c43d73d1e/ea4ab/mlic.jpg","srcSet":"/static/6fb008f2b044681631d2196c43d73d1e/477ba/mlic.jpg 175w,\n/static/6fb008f2b044681631d2196c43d73d1e/06776/mlic.jpg 350w,\n/static/6fb008f2b044681631d2196c43d73d1e/ea4ab/mlic.jpg 700w,\n/static/6fb008f2b044681631d2196c43d73d1e/61d2c/mlic.jpg 1024w","srcWebp":"/static/6fb008f2b044681631d2196c43d73d1e/89afa/mlic.webp","srcSetWebp":"/static/6fb008f2b044681631d2196c43d73d1e/9fca7/mlic.webp 175w,\n/static/6fb008f2b044681631d2196c43d73d1e/37a4e/mlic.webp 350w,\n/static/6fb008f2b044681631d2196c43d73d1e/89afa/mlic.webp 700w,\n/static/6fb008f2b044681631d2196c43d73d1e/b5e5f/mlic.webp 1024w","sizes":"(max-width: 700px) 100vw, 700px"}}},"tech":["OpenCV","TensorFlow","Python","Flask"],"github":"https://github.com/dravinash/course_recommendation_system","external":"https://www.youtube.com/watch?v=WA_zHBOXka8"},"html":"<p>We trained a CNN based multi label classification system to recognize 9605 classes that includes different scenes, actions, object and labels. The system gained precision, recall and f1 score of 57.34%, 89.63% and 66.65%.</p>"}},{"node":{"frontmatter":{"title":"Understandable Robot Teams","cover":{"childImageSharp":{"fluid":{"tracedSVG":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAABJ0AAASdAHeZh94AAACiklEQVQozwXBC08SAQAA4PtNrba22prRzJyzrTlnrjZrubSltYWGBiIiSmKgICUnyFveHMfT4+Rxch5wwAH34h6Af6XvAwa8xLLicDhqETfdNs6yXUGgRZGWBGoks/KdR0xv84kNJvi14/qM/ZkjzZNNy1RKowgpnwJ8707k2kOZpUis12uyA1oQWUnmZYm+H3PjukeGlWLiBx9c6zk/Eabp7qmCPJlKaxWQbhoo5Xw1NEo2qr1uneO6gsSLIivL/FDsyxIzEGguZxKuVgfe5b7jPQUuMbYXxPFkZmciZ1oEcDTcvIV6HZyiSJqlRZkThJ4k9GWOFHmK5nghvSe6FnjP8riTl4Yihfpw01Ra9aRoXwVqSKBRjfZaZbJxm8D4HNbtkJjENSSGIG6RUMh/7day4BvOvdTi78vUqIKTFfNcevNR2a0CbvPuRumKqhdIAg2kKoFYto0jAxprN8tGV/QimnP7w82Mg/N/qVTKyRJTiv4taR7APx9XgnqgAjuIa0+7BjXqWBxKX55bETjA0xiFeE2mvROH1bTzvZb8RyKhq6AvARcLmXTJ+gFWPsRiJ0AhbsMLrhIS8caS4SRkBr1nfpgoRe69qjFiZIrrTUhXC347WnmmmF1wOC4wkksmoZT2VSVqATJnasK+ntN/BI2m0+0t0HBg1lryoHaA7jNZHQup0Jha61DPzEy8nHnt9kcCEfjSF4kcrRRDR8DwEhzta0a6LdFg4HS7kkEv7e/R0YNWzhixbWrW5on4YT/+61A5v6vThqKpc6cnnMiGLBtZjx6oxi13RSdWTuVRFL2BA8lEuVbmqhdj3B60bi7OPmeubWxG146oEwHw2Gx32n5DGTSw+y52+PY/p+e4irYO5VcAAAAASUVORK5CYII=","aspectRatio":1.7857142857142858,"src":"/static/8947d88e8332fe6e23bbc86f35f89683/60290/robot_teaming.png","srcSet":"/static/8947d88e8332fe6e23bbc86f35f89683/847ef/robot_teaming.png 175w,\n/static/8947d88e8332fe6e23bbc86f35f89683/91cba/robot_teaming.png 350w,\n/static/8947d88e8332fe6e23bbc86f35f89683/60290/robot_teaming.png 700w,\n/static/8947d88e8332fe6e23bbc86f35f89683/f5f50/robot_teaming.png 1050w,\n/static/8947d88e8332fe6e23bbc86f35f89683/38674/robot_teaming.png 1400w,\n/static/8947d88e8332fe6e23bbc86f35f89683/c608a/robot_teaming.png 1920w","srcWebp":"/static/8947d88e8332fe6e23bbc86f35f89683/89afa/robot_teaming.webp","srcSetWebp":"/static/8947d88e8332fe6e23bbc86f35f89683/9fca7/robot_teaming.webp 175w,\n/static/8947d88e8332fe6e23bbc86f35f89683/37a4e/robot_teaming.webp 350w,\n/static/8947d88e8332fe6e23bbc86f35f89683/89afa/robot_teaming.webp 700w,\n/static/8947d88e8332fe6e23bbc86f35f89683/78e7a/robot_teaming.webp 1050w,\n/static/8947d88e8332fe6e23bbc86f35f89683/03d34/robot_teaming.webp 1400w,\n/static/8947d88e8332fe6e23bbc86f35f89683/6833b/robot_teaming.webp 1920w","sizes":"(max-width: 700px) 100vw, 700px"}}},"tech":["OpenCV","TensorFlow","Keras","Python","Html/CSS/JS"],"github":"","external":"https://www.youtube.com/watch?v=WA_zHBOXka8"},"html":"<p>We created a framework that helps pepper robot to convert the actions into the natural language which makes them understandable. The framework also supports to collaborate them as a team to solve the problem.</p>"}},{"node":{"frontmatter":{"title":"Human Robot Collaboration","cover":{"childImageSharp":{"fluid":{"tracedSVG":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAAA7DAAAOwwHHb6hkAAADKklEQVQozx3DWUyTBwDA8e+B7UEHc8Zl0zQbG+keGMRihjpQt4jMjYAXoehqaTqogxgsCZeaHYDFVGEcplAqCINSNqBahFK/cpTv63qhlGMIOgIsZi57WPRhx8Oe/sv2S35CbWs99tHvcHjuMDU3y3QkiGN4AFtvF02WRtp7OrD125kMBln5+QnygygTwTBWey81DWbMbRbMjY1cLC+j4kIJQtO3Vm70d9I20E3bQA82Ry/aczpKykooqzLS0tGOuaWR2obr3OjuwzboxB2IMOLzIy09ZHZljWvvHqI2Np7abfEI40GJsYD8f5cssbixwfL6Gi03b5KrUfPm26+hTEogM+cooZV1Np79xeazP3jy/E9+/fsf1tZ/4ZvtidTH7MK09Q2EjaebrGyu8fDpb/iXH+GNRBl0i4yOuehoNXP8xEkOpKVQWqwhtPAjwfklwv+NLhJdfYzvrogpNoErLyqo3rITIbK8RHRO4qffn/NFczv6z3RYaqoY67iKtbKIavVprn1pZGKoC3HGz4QUwBcIIwUj3F99zHing6+E1zHFKDgftxNBlj2Me4Zx+iTOGi+RcewUeVo9nZYmLhfn845SwUVDHn1t17k3PYNXCjAph5mUQwSWH3G7sZ16RTL1rypRx+5AqLls4MqlQgr1uaQdOUJ2/hk0hTqO52ahUr2FYlcc5zVHaTFV0fu9k+FREfdUgPFJP/LiKl3VdVSo9mNUJJC75WWEcu0+ynVp5J1IR5V2iPTDGWiLjGRkF6FQpnMwVcmt5kp6rHU4+m04XcPcFacYESXcoQXMagOGHfFoXtiGdusrCAZ9JvqCw+Tk7Gf3x2dJ+rCU+MQzJO0tRpl8iqzMFLzOZnzjnUheOz7vAFPiIF7xNqNjTkznivn8vQN8GredgpiXEAq0GRhL8yjUf4QqI4s9n1SwO1XLnvcLSU45RuWFfCZcFmRvPxHZxazfxf0fRgnLLoJTQ9wb6aaz7So2UzkNRacRrC0V2G99jamumL0fHCR5Xx6q1HwSVWqys3W4h5qRRDuBaScR/whzITfzEQ/zYQ8LITdR/xAPZvoITnQj3mnlX0qaOe7FX7G7AAAAAElFTkSuQmCC","aspectRatio":1.7857142857142858,"src":"/static/57c74f3314ed9e9da3a97c878a26fb50/60290/human_robot_collaboration.png","srcSet":"/static/57c74f3314ed9e9da3a97c878a26fb50/847ef/human_robot_collaboration.png 175w,\n/static/57c74f3314ed9e9da3a97c878a26fb50/91cba/human_robot_collaboration.png 350w,\n/static/57c74f3314ed9e9da3a97c878a26fb50/60290/human_robot_collaboration.png 700w,\n/static/57c74f3314ed9e9da3a97c878a26fb50/f5f50/human_robot_collaboration.png 1050w,\n/static/57c74f3314ed9e9da3a97c878a26fb50/38674/human_robot_collaboration.png 1400w,\n/static/57c74f3314ed9e9da3a97c878a26fb50/c608a/human_robot_collaboration.png 1920w","srcWebp":"/static/57c74f3314ed9e9da3a97c878a26fb50/89afa/human_robot_collaboration.webp","srcSetWebp":"/static/57c74f3314ed9e9da3a97c878a26fb50/9fca7/human_robot_collaboration.webp 175w,\n/static/57c74f3314ed9e9da3a97c878a26fb50/37a4e/human_robot_collaboration.webp 350w,\n/static/57c74f3314ed9e9da3a97c878a26fb50/89afa/human_robot_collaboration.webp 700w,\n/static/57c74f3314ed9e9da3a97c878a26fb50/78e7a/human_robot_collaboration.webp 1050w,\n/static/57c74f3314ed9e9da3a97c878a26fb50/03d34/human_robot_collaboration.webp 1400w,\n/static/57c74f3314ed9e9da3a97c878a26fb50/6833b/human_robot_collaboration.webp 1920w","sizes":"(max-width: 700px) 100vw, 700px"}}},"tech":["OpenCV","TensorFlow","Mocap"],"github":"","external":"https://www.youtube.com/watch?v=2GNWYsOfDYw"},"html":"<p>In order to have a safe and reliable human robot collaboration we need to recognize human actions. We trained a RNN base deep neural network on the motion capture suit data that helps robot to recognize human actions.</p>"}}]},"projects":{"edges":[{"node":{"frontmatter":{"title":"Natural Language to Python","tech":["Transformers","BERT","Rest API"],"github":"https://github.com/dravinash/course_recommendation_system","external":"https://www.youtube.com/watch?v=WA_zHBOXka8"},"html":"<p>Helps programmar to generate code given the query in English.</p>"}},{"node":{"frontmatter":{"title":"Large Scale Object Detection","tech":["Faster RCNN","MobileNet","Yolo"],"github":"https://github.com/dravinash/course_recommendation_system","external":"https://www.youtube.com/watch?v=WA_zHBOXka8"},"html":"<p>We trained Faster RCNN to recognize 617 objects collected in house.</p>"}},{"node":{"frontmatter":{"title":"Incremental Face Recognition","tech":["Distillation Loss","CNN","FaceNet"],"github":"https://github.com/dravinash/course_recommendation_system","external":"https://www.youtube.com/watch?v=WA_zHBOXka8"},"html":"<p>Used distillation loss to filter out the knowledge and train the neural network without utilizing the whole dataset rather taking a subsample.</p>"}},{"node":{"frontmatter":{"title":"KYC Document Processing","tech":["OCR","Faster RCNN","NLP"],"github":"https://github.com/dravinash/course_recommendation_system","external":"https://www.youtube.com/watch?v=WA_zHBOXka8"},"html":"<p>Helps to process PAN, AADHAAR, Driving Licence, Passport and Voter ID Card and extract required fields.</p>"}},{"node":{"frontmatter":{"title":"Optical Character Recognition","tech":["CTPN","Tesseract","Auto Encoders"],"github":"https://github.com/dravinash/course_recommendation_system","external":"https://www.youtube.com/watch?v=WA_zHBOXka8"},"html":"<p>Trained a CTPN model for text detection, customized tesseract for text conversion. Further trained a auto encoders for the post ocr error correction.</p>"}},{"node":{"frontmatter":{"title":"Invoice Processing","tech":["Text Detection","Faster RCNN","Tessaract"],"github":"https://github.com/dravinash/course_recommendation_system","external":"https://www.youtube.com/watch?v=WA_zHBOXka8"},"html":"<p>Trained a Faster RCNN to localize invoice fields, further complement with the help of text detection and the tessract based information extraction system.</p>"}},{"node":{"frontmatter":{"title":"Intelligent Character Recognition","tech":["CNN","LSTM","TensorFlow"],"github":"https://github.com/dravinash/course_recommendation_system","external":"https://www.youtube.com/watch?v=WA_zHBOXka8"},"html":"<p>Retrained an existing opensource solution OCROPY to recognize hand written text.</p>"}},{"node":{"frontmatter":{"title":"Legal Document Processing","tech":["TF/IDF","Word2Vec","Tensorflow"],"github":"https://github.com/dravinash/course_recommendation_system","external":"https://www.youtube.com/watch?v=WA_zHBOXka8"},"html":"<p>Used NLP to extract the start and end marker of a section to derive information from the docuemnt. We have used legal (contract) documents to design the solution.</p>"}},{"node":{"frontmatter":{"title":"Scanned Document Processing","tech":["OpenCV","NLTK","Tensorflow"],"github":"https://github.com/dravinash/course_recommendation_system","external":"https://www.youtube.com/watch?v=WA_zHBOXka8"},"html":"<p>We used computer vision and natural language process to digitize the medical prescription.</p>"}}]},"contact":{"edges":[{"node":{"frontmatter":{"title":"Get In Touch","buttonText":"Mail Me"},"html":"<p>My inbox is always open. Whether you have a question or just want to say hello, I'll try my best to get back to you! Feel free to mail me about any relevant job updates.</p>"}}]}},"pageContext":{}},"staticQueryHashes":["3115057458"]}