{"componentChunkName":"component---src-pages-blog-index-js","path":"/blog/","result":{"data":{"allMarkdownRemark":{"edges":[{"node":{"frontmatter":{"title":"Attention Please!","description":"Attention in machine learning is like the brain's spotlight, highlighting important parts of data. Introduced in 2014, it focuses on relevant information using \"queries,\" \"keys,\" and \"values.\" Imagine a model yelling, \"Hey, brain, remember that detail!\" to handle complex tasks like translating long sentences.","slug":"/blog_posts/attention","date":"2024-07-22","tags":["CNN","Attention","TensorFlow"],"draft":false},"html":"<h2>Overview</h2>\n<p>This article would discuss the following concepts</p>\n<blockquote>\n<ol>\n<li>Basic intuation of attention model</li>\n<li>Mathematics behind the attention model</li>\n<li>Implementation of attention mechanism</li>\n<li>Use of Tensorflow <code class=\"language-text\">tf.keras.layers.MultiHeadAttention</code> API</li>\n<li>Multi-Head Attention</li>\n</ol>\n</blockquote>\n<p>Without further delay, lets deep dive into what is attention mechanism, why it is so popular?</p>\n<h2>Basic intuation of attention model</h2>\n<p>Imagine you are reading a book, and you come across a character whose actions are essential to understand the current plot. To make sense of what's happening, you need to remember previous details about this character. Here’s how the attention mechanism works in this context: <br>\n<font color=#64ffda><strong>Query:</strong></font> Think of the query as the question you're asking while reading. For example, \"What has this character done before?\" <br>\n<font color=#64ffda><strong>Key:</strong></font> The key is like an index or a set of clues that help you find the right information in the book. It's the reference to different points in the story where this character is mentioned. <br>\n<font color=#64ffda><strong>Value:</strong></font> The value is the actual information you need about the character, like their past actions and traits.</p>\n<p>If we explain the same example in terms of python data types then it would be something similar to as described in the below example. The query is the question that we are looking for, key is the reference that can be <code class=\"language-text\">mahabharata_characters</code> and value would be the value of the query. So if we are looking for the character <code class=\"language-text\">Ashwatthama</code> then the <code class=\"language-text\">mahabharata_characters</code> would be the reference, treated as key and the value as <code class=\"language-text\">The formidable warrior and son of Dronacharya, known for his unrelenting pursuit of revenge against the Pandavas.</code>. <br></p>\n<div class=\"gatsby-code-title\">Dictionary.py</div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># dictionary of characters and their description</span>\nmahabharata_characters <span class=\"token operator\">=</span> <span class=\"token punctuation\">{</span>\n    <span class=\"token string\">\"Arjuna\"</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"The peerless archer, whose valor and skill were unmatched on the battlefield of Kurukshetra\"</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"Krishna\"</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"The divine charioteer and guide to Arjuna, whose wisdom and counsel in the Bhagavad Gita are legendary.\"</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"Bhishma\"</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"The grand patriarch of the Kuru dynasty, renowned for his vow of celibacy and unparalleled warrior prowess.\"</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"Draupadi\"</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"The fiery princess and wife of the Pandavas, whose dignity and strength became a rallying point in their quest for justice.\"</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"Karna\"</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"The tragic hero and unparalleled warrior, known for his unwavering loyalty to Duryodhana and his inner conflict regarding his true lineage.\"</span>\n    <span class=\"token string\">\"Ashwatthama\"</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"The formidable warrior and son of Dronacharya, known for his unrelenting pursuit of revenge against the Pandavas.\"</span>\n<span class=\"token punctuation\">}</span></code></pre></div>\n<p>Build upon this intuation, attention mechanism keeps the track of long sequences and give more weightsage to only certain portion who are useful for the token.</p>\n<h2>Mathematics behind the attention model</h2>\n<p>If we see the architecture proposed in the paper<sup id=\"fnref-1\"><a href=\"#fn-1\" class=\"footnote-ref\">1</a></sup>, it has the following component <code class=\"language-text\">Scale</code>, <code class=\"language-text\">Mask (Opt)</code>, <code class=\"language-text\">Softmax</code> and <code class=\"language-text\">matmul</code>. These are expressed in the hand written notes.\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 562px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/97fcca73755a96557680f3a08c26b760/eef63/attention-1.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 53.714285714285715%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAABJ0AAASdAHeZh94AAAByUlEQVQoz3VTa0/iQBTl//+DdTfx8cFoIg/JCqsiSrHgCoIUfEBBUgp9EMqj25a+z2ZGqyhyksm5c3tzeubemZgsyxBFEZIkYSAMQPYkFgQBpmmCIAzD90WgqipGoxGtVRSF1mqaRr/FbNuGZVlwHBtB6MMPPIQIqJjneWuCZHmuB3WsotHkqNByuYTjOK+CeINtuSgXOWTSDBr1dpR+dxVxEASUJVnBef4Skzdn0c+ooOu5aLYfcJBM4Yq5wdMDT0rWjhoxgSI/osUlMB53sQoqOJ/PkL++wGkhi57A0+N/dRfFuq6j0+6hydVxfnaM+3oNPN/HUJSoeyo4Uye4O2Fwn2HBV1uYzmbf9o7ghm3hYK+A7a0Mtn9ksfMzi/h+CTu/ktC0CWJRYYPLocDsYqIp0PV/Gx12OwP8+c2iVukgdXSG2/IjLk4rKLFV+L736tCybDBXLFLxNHpdAYZhbBR87jzjMH6IfCGP+HECucscEukkiqXrjx4ahom/ZQ6lYgNdXoBhbhYUh0NUa3d46fdxW6lQJnuSp4JRoe+7dLJBGMBx3LWproov5nMaLxYLylNt+nnK312Lr/HqPSSPgTB5ECRPWhTdz/+EAzw32UT3cwAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"attention 1\"\n        title=\"attention 1\"\n        src=\"/static/97fcca73755a96557680f3a08c26b760/eef63/attention-1.png\"\n        srcset=\"/static/97fcca73755a96557680f3a08c26b760/1aaec/attention-1.png 175w,\n/static/97fcca73755a96557680f3a08c26b760/98287/attention-1.png 350w,\n/static/97fcca73755a96557680f3a08c26b760/eef63/attention-1.png 562w\"\n        sizes=\"(max-width: 562px) 100vw, 562px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span>\nThe scaling or normalization is performed as if the value of dk become a large number, then the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients. To counteract this effect, we scale the dot products by <code class=\"language-text\">1/sqrt(dk)</code> <sup id=\"fnref-1\"><a href=\"#fn-1\" class=\"footnote-ref\">1</a></sup>. The mask operation is optional, this one is used, in sequence to sequence generation or similar application. When the objective of the model is to predict the next token (word), and the input feature is the collection of previous words and the next word. If we want to mask the future token, in that situation we can use this masking operation. This sets the upper tringular matrix of the dot product between query and key to infinte and when we apply softmax on top of this, the upper tringular matrix is set to zero. This we do to make the sum equal to 1. For more reference, follow this video <sup id=\"fnref-2\"><a href=\"#fn-2\" class=\"footnote-ref\">2</a></sup>.\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 700px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/fe71b6a03016d0546283780acee4aeb2/9485b/attention-2.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 140%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAcCAYAAABh2p9gAAAACXBIWXMAAAsTAAALEwEAmpwYAAADyElEQVRIx41W2ZajOgwUOwbMHiAECMlkpvux///nak6JONN9eu7yoOMFuVwuyTKS5zmWZcG6rqjrGtfrFbfbDWVZQkT+1TzPg+/7X/tJkmAYBrRtq4Dsn89n7RM0yzI1a61+a5rmNXZgRVEgjuNjoyAIUFUVjDGY51k/nk4nbTkexxF932vL09CPLdewjaJI/dI0PQBJlRN0eH9/1+M7QLZkTsY0sqI07JMtfdlyvWMrjjKdCEAN2XL3aZp0TFAaNyVA13XYtk3ByZ7rSewFyIU06vZfgfgfJmBgeBy2FJzBIEOOuTv7ZMh5GgNDtiThtCVz6qsaUlAOCMYPLhAE40KOaTwB5aA8BKUcHx8f6rPvOxhgZUgnasIFBCIwATjPzSg6T1DaEpfLBUVewKRG+wTmmv26I3SAjBAneYQwDDUVyJgtWfKY23VDc2phmxJVW2FeZ9RdjeW6oD21CNLwa1CoEVk4zbiBpsdpwM9fP7GeVyzJhEsyYi1mbPaC1q+w+BNG6TBIi06qA/CyLKjqGlVdqR5kRlZMDzJmPwoiGEmQi0HlWaSSIAsMMt+g8i3GuEfixQcgdWrLBnmaoy4P4U2SqmYu6ra0WLYFtrLoxx5VU2t/micM5xG3xw2FLQ5ALlzzGVMxYDYjispiyc6YsgFlXaJr21eKEJzydG2Hx+OhWcGAcuOXhtQstzlMbjQYdKIDo0kJGH3O86pxnidiS2CXcpqDDpDm8s4FIy+OwKgcz0pEEBdAtk5n+pP9C9CljSbxMOJ+vysQnd0R+Z1jAvMOk+22bqo3/VlDn/f5KI5d3yErMvSnHvtt10gzELYqYZ71zxUIMtVk3ncFJgmyfAHSYolQeBliL9J+ID5MZGDTApnJ9EaQHYGzPEdZlSjMIUsYh1+LAy2TFJUUmmuFZEglRimFzvVSoxaLWU7aZxJP0uMkjY5/yIrOq78C6tHFg5Vck5cbhBIgkhC+eNoPJIAvPjIxSL1E13BMH679Bmgk1WvUSY2LDMqExg12ueAuC1qpsMuMTc56kk9A3wH/8XUTD4nECOXQaknP+JFeNRO8Z1D/CsiFi4zKapUzRq/TozJID9mUNXW9pBPGpEcb1TiXI+I0+QqoJSuOYHOLIi0QBqHeEkZWK3qaIvQDreBJkGhAVm9C6AWIgxjiiabZ86F6Xr08R9M2WnHYZ1FlbjEt3JVjQWVurtOC+3pD27W47scjxgR/vs1/jsyr5x5y5hwdqJN7CrgRQcdpxLKt+gS8vb3p/F81/Pxb4eae78Q30wR/ForP334DwUZ+nbzkyk0AAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"attention 2\"\n        title=\"attention 2\"\n        src=\"/static/fe71b6a03016d0546283780acee4aeb2/39600/attention-2.png\"\n        srcset=\"/static/fe71b6a03016d0546283780acee4aeb2/1aaec/attention-2.png 175w,\n/static/fe71b6a03016d0546283780acee4aeb2/98287/attention-2.png 350w,\n/static/fe71b6a03016d0546283780acee4aeb2/39600/attention-2.png 700w,\n/static/fe71b6a03016d0546283780acee4aeb2/57cd1/attention-2.png 1050w,\n/static/fe71b6a03016d0546283780acee4aeb2/9485b/attention-2.png 1155w\"\n        sizes=\"(max-width: 700px) 100vw, 700px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span>\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 700px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/f4923867f5990a11beb3746736fcf62a/2b633/attention-3.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 69.71428571428572%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAYAAAAvxDzwAAAACXBIWXMAAAsTAAALEwEAmpwYAAACFklEQVQ4y52UW0/cMBCFjy+xE9tJNlc2LF0QSlHbl1YqbUXLC///R51qYnYBib70YeJ4zvjzsTIOYAOhPaEDAcNt7hrClISNhO8I1xJFypqRsck531KVfa4DnsNGKp9YDHuqMGTRJqLaUTUTUUqMhEtEGKnnI3W/J7TjeveZ3+/v2XX9KyDAZUi8WRqmQZwV1ADjrqUeRupQEWXJUCdOU0eVaqKuWTeJj39+8+npiftl/wKMMXJd7/jp5shhatgMMy+miS7WdOOBSltq41k4x7Iqt0XetfTtyB+/fvLh8YHXh2ve4ooHzIQxhpeHA68WAWXrMQa2zUhfdCy0p0XJFBO7bkcFw1DsCGW5riu/ffnKu/qWH82Ro+nzkZ0raK0loM7Wu9hztPP2bqCpoRhDoDb2XJMQuOLIS0ycuo6mLDLwbWRoo2vOqmcJzxqRARXnYWIoK3q4LS+5AgVb1IyoqKHfA+YoAA5oaV65llDA9tHUP9YB2uYek77zu+f+6olyILTJedGLmqimPLo696q0k2hSr8zphJYI+ywKpJpzQbp6AcsmAhDQpncZLgZOG2l3AmoiXBBxyYUC325JyIWnkBshUAHW10T68Jwf8uYynoHibtuxJ8KSRTmKOEsHornNerwkqjEDxYDMZa04Nf7lpmyCuBSH4myLKh9H4FIszjbokjWBCFhcy9qzQ3mcfgLNzZte/J/4C7AnMzQPwW6EAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"attention 3\"\n        title=\"attention 3\"\n        src=\"/static/f4923867f5990a11beb3746736fcf62a/39600/attention-3.png\"\n        srcset=\"/static/f4923867f5990a11beb3746736fcf62a/1aaec/attention-3.png 175w,\n/static/f4923867f5990a11beb3746736fcf62a/98287/attention-3.png 350w,\n/static/f4923867f5990a11beb3746736fcf62a/39600/attention-3.png 700w,\n/static/f4923867f5990a11beb3746736fcf62a/57cd1/attention-3.png 1050w,\n/static/f4923867f5990a11beb3746736fcf62a/4af54/attention-3.png 1400w,\n/static/f4923867f5990a11beb3746736fcf62a/2b633/attention-3.png 1749w\"\n        sizes=\"(max-width: 700px) 100vw, 700px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<h2>Implementation of attention mechanism</h2>\n<p>This section is the implementation of the above section in Tensorflow, I have taken reference from Professor Mubarak Shah Lecture, you can visit for more details <sup id=\"fnref-3\"><a href=\"#fn-3\" class=\"footnote-ref\">3</a></sup>.</p>\n<div class=\"gatsby-code-title\">attention_model.py</div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">input_shape <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span><span class=\"token number\">224</span><span class=\"token punctuation\">,</span> <span class=\"token number\">224</span><span class=\"token punctuation\">,</span> <span class=\"token number\">3</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\"># Assuming a 3-channel (RGB) image</span>\n\n<span class=\"token comment\"># Define the input layer</span>\ninputs <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>keras<span class=\"token punctuation\">.</span>Input<span class=\"token punctuation\">(</span>shape<span class=\"token operator\">=</span>input_shape<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># Add a pre-trained ResNet50 layer (excluding the top classification layer)</span>\nresnet50_base <span class=\"token operator\">=</span> applications<span class=\"token punctuation\">.</span>ResNet50<span class=\"token punctuation\">(</span>include_top<span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">,</span> weights<span class=\"token operator\">=</span><span class=\"token string\">'imagenet'</span><span class=\"token punctuation\">,</span> input_tensor<span class=\"token operator\">=</span>inputs<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># number of filters</span>\ndq <span class=\"token operator\">=</span> dk <span class=\"token operator\">=</span> dv <span class=\"token operator\">=</span><span class=\"token number\">3</span>\nfeature_dimension <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span>resnet50_base<span class=\"token punctuation\">.</span>output_shape<span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> resnet50_base<span class=\"token punctuation\">.</span>output_shape<span class=\"token punctuation\">[</span><span class=\"token number\">2</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> dk<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"Output feature dimension:\"</span><span class=\"token punctuation\">,</span>feature_dimension<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># Add a 1x1 convolutional layer to get the query</span>\nquery <span class=\"token operator\">=</span> layers<span class=\"token punctuation\">.</span>Conv2D<span class=\"token punctuation\">(</span>filters<span class=\"token operator\">=</span>dq<span class=\"token punctuation\">,</span> kernel_size<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> activation<span class=\"token operator\">=</span><span class=\"token string\">'relu'</span><span class=\"token punctuation\">,</span> padding<span class=\"token operator\">=</span><span class=\"token string\">'same'</span><span class=\"token punctuation\">,</span> name<span class=\"token operator\">=</span><span class=\"token string\">\"query\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>resnet50_base<span class=\"token punctuation\">.</span>output<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"Query output shape:\"</span><span class=\"token punctuation\">,</span> query<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># Add a 1x1 convolutional layer to get the key</span>\nkey <span class=\"token operator\">=</span> layers<span class=\"token punctuation\">.</span>Conv2D<span class=\"token punctuation\">(</span>filters<span class=\"token operator\">=</span>dk<span class=\"token punctuation\">,</span> kernel_size<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> activation<span class=\"token operator\">=</span><span class=\"token string\">'relu'</span><span class=\"token punctuation\">,</span> padding<span class=\"token operator\">=</span><span class=\"token string\">'same'</span><span class=\"token punctuation\">,</span> name<span class=\"token operator\">=</span><span class=\"token string\">\"key\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>resnet50_base<span class=\"token punctuation\">.</span>output<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"Key output shape:\"</span><span class=\"token punctuation\">,</span> key<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># Add a 1x1 convolutional layer to get the value</span>\nvalue <span class=\"token operator\">=</span> layers<span class=\"token punctuation\">.</span>Conv2D<span class=\"token punctuation\">(</span>filters<span class=\"token operator\">=</span>dv<span class=\"token punctuation\">,</span> kernel_size<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> activation<span class=\"token operator\">=</span><span class=\"token string\">'relu'</span><span class=\"token punctuation\">,</span> padding<span class=\"token operator\">=</span><span class=\"token string\">'same'</span><span class=\"token punctuation\">,</span> name<span class=\"token operator\">=</span><span class=\"token string\">\"value\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>resnet50_base<span class=\"token punctuation\">.</span>output<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"Value output shape:\"</span><span class=\"token punctuation\">,</span> value<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># Flatten the conv_layer to a 2D tensor</span>\nflattened_query <span class=\"token operator\">=</span> layers<span class=\"token punctuation\">.</span>Reshape<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">3</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>query<span class=\"token punctuation\">)</span>\nflattened_key <span class=\"token operator\">=</span> layers<span class=\"token punctuation\">.</span>Reshape<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">3</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>key<span class=\"token punctuation\">)</span>\nflattened_value <span class=\"token operator\">=</span> layers<span class=\"token punctuation\">.</span>Reshape<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">3</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>value<span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"Flattened Query shape:\"</span><span class=\"token punctuation\">,</span> flattened_query<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"Flattened Key shape:\"</span><span class=\"token punctuation\">,</span> flattened_key<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"Flattened Value shape:\"</span><span class=\"token punctuation\">,</span> flattened_value<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># Transpose the flattened_key</span>\ntransposed_key <span class=\"token operator\">=</span> layers<span class=\"token punctuation\">.</span>Lambda<span class=\"token punctuation\">(</span><span class=\"token keyword\">lambda</span> x<span class=\"token punctuation\">:</span> tf<span class=\"token punctuation\">.</span>transpose<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">,</span> perm<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>flattened_key<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># Print the shape of the transposed_key</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"Transposed Key shape:\"</span><span class=\"token punctuation\">,</span> transposed_key<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># Compute the dot product between the query and the key and scale it</span>\nscaled_prod <span class=\"token operator\">=</span> layers<span class=\"token punctuation\">.</span>Dot<span class=\"token punctuation\">(</span>axes<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>flattened_query<span class=\"token punctuation\">,</span> transposed_key<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">/</span> tf<span class=\"token punctuation\">.</span>math<span class=\"token punctuation\">.</span>sqrt<span class=\"token punctuation\">(</span>tf<span class=\"token punctuation\">.</span>cast<span class=\"token punctuation\">(</span>dk<span class=\"token punctuation\">,</span> tf<span class=\"token punctuation\">.</span>float32<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"Dot Product shape:\"</span><span class=\"token punctuation\">,</span> prod<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># Apply softmax to the attention scores</span>\nsoftmax_layer <span class=\"token operator\">=</span> layers<span class=\"token punctuation\">.</span>Softmax<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>scaled_prod<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"Softmax shape:\"</span><span class=\"token punctuation\">,</span> softmax_layer<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># Compute the attention-weighted value</span>\nattention_output <span class=\"token operator\">=</span> layers<span class=\"token punctuation\">.</span>Dot<span class=\"token punctuation\">(</span>axes<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>softmax_layer<span class=\"token punctuation\">,</span> flattened_value<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"Attention Output shape:\"</span><span class=\"token punctuation\">,</span> attention_output<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">)</span>\n\nreshaped_attention_output <span class=\"token operator\">=</span> layers<span class=\"token punctuation\">.</span>Reshape<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>feature_dimension<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>attention_output<span class=\"token punctuation\">)</span>\n\n\n<span class=\"token comment\"># # Reshape the attention output to match the original image dimensions</span>\n<span class=\"token comment\"># reshaped_attention_output = layers.Reshape((32, 32, 3))(attention_output)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"Reshaped Attention Output shape:\"</span><span class=\"token punctuation\">,</span> reshaped_attention_output<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># Apply a 1x1 convolutional layer to the attention output</span>\nself_attended_feature <span class=\"token operator\">=</span> layers<span class=\"token punctuation\">.</span>Conv2D<span class=\"token punctuation\">(</span>filters<span class=\"token operator\">=</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span> kernel_size<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> activation<span class=\"token operator\">=</span><span class=\"token string\">'relu'</span><span class=\"token punctuation\">,</span> padding<span class=\"token operator\">=</span><span class=\"token string\">'same'</span><span class=\"token punctuation\">,</span> \n                                      name<span class=\"token operator\">=</span><span class=\"token string\">\"feature_map\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>reshaped_attention_output<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"Self-attended Feature shape:\"</span><span class=\"token punctuation\">,</span> self_attended_feature<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">)</span></code></pre></div>\n<h2>Use of Tensorflow <code class=\"language-text\">tf.keras.layers.MultiHeadAttention</code> API</h2>\n<p>If you are using Tensoflow then, you may use <code class=\"language-text\">tf.keras.layers.MultiHeadAttention</code> this API to use the multihead attention directly. If <code class=\"language-text\">query</code>, <code class=\"language-text\">key</code>, <code class=\"language-text\">value</code> are the same, then this is self-attention <sup id=\"fnref-4\"><a href=\"#fn-4\" class=\"footnote-ref\">4</a></sup>. Each timestep in query attends to the corresponding sequence in <code class=\"language-text\">key</code>, and returns a fixed-width vector.</p>\n<p>This layer first projects <code class=\"language-text\">query</code>, <code class=\"language-text\">key</code> and <code class=\"language-text\">value</code>. These are (effectively) a list of tensors of length <code class=\"language-text\">num_attention_heads</code>, where the corresponding shapes are (<code class=\"language-text\">batch_size</code>, <code class=\"language-text\">&lt;query dimensions></code>, <code class=\"language-text\">key_dim</code>), (<code class=\"language-text\">batch_size</code>, <code class=\"language-text\">&lt;key/value dimensions></code>, <code class=\"language-text\">key_dim</code>), (<code class=\"language-text\">batch_size</code>, <code class=\"language-text\">&lt;key/value dimensions></code>, <code class=\"language-text\">value_dim</code>).</p>\n<p>Then, the query and key tensors are dot-producted and scaled. These are softmaxed to obtain attention probabilities. The value tensors are then interpolated by these probabilities, then concatenated back to a single tensor. Finally, the result tensor with the last dimension as value_dim can take a linear projection and return.</p>\n<div class=\"gatsby-code-title\">multihead_attention_model.py</div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">self_attended_feature <span class=\"token operator\">=</span> layers<span class=\"token punctuation\">.</span>MultiHeadAttention<span class=\"token punctuation\">(</span>num_heads<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> key_dim<span class=\"token operator\">=</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span> value_dim <span class=\"token operator\">=</span> <span class=\"token number\">3</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>resnet50_base<span class=\"token punctuation\">.</span>output<span class=\"token punctuation\">,</span> resnet50_base<span class=\"token punctuation\">.</span>output<span class=\"token punctuation\">)</span></code></pre></div>\n<p>The comparison between both the models can be seen in the below figure.\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 700px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/0df7fd6cb7a86333f53b0b10f85f6633/faabe/attention-4.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 40%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAYAAAD5nd/tAAAACXBIWXMAABJ0AAASdAHeZh94AAABUUlEQVQoz1WRi46rMAxE+/9/2WopkAdJnBeUkLOCXt12LY1iWbJnMnOblcZYh/UR5Tw2WESElBLaaoZhYFIjr/3FWb13en+/0L9mb9zGaWZeMrVU5iD4FCilUtaCMjPTPCExsO87n+qfrne+66a0ZgmJlAtGhJACOWfWdcUYzf3nTqnlmmlj8SEhuV7EuWTqWnHeobRCknDT2mBdRGLBnArj++B5xFjNOI2EGBAJ3AeFcYlFIjY6Ss7ELDynJ/fHAxeWU6HB+kytFf1P4fnldVuxi+HnORBzukiUmrFpwQXPtu+04+DV2uXvaUlr7VSoL9ZcVmyM13Kple21YRbDY3hcarft9Z9o9Z4jCD3GC7T28XCaJkbjEYloHwhRSGfKUdBqYnw+kSh/QzkOemv0fX/jO2XnHD55pAo5JfpxcKTEESMtCMe2/Vm48CfnT9rn7Bc5Y2pOl41uTgAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"attention 4\"\n        title=\"attention 4\"\n        src=\"/static/0df7fd6cb7a86333f53b0b10f85f6633/39600/attention-4.png\"\n        srcset=\"/static/0df7fd6cb7a86333f53b0b10f85f6633/1aaec/attention-4.png 175w,\n/static/0df7fd6cb7a86333f53b0b10f85f6633/98287/attention-4.png 350w,\n/static/0df7fd6cb7a86333f53b0b10f85f6633/39600/attention-4.png 700w,\n/static/0df7fd6cb7a86333f53b0b10f85f6633/57cd1/attention-4.png 1050w,\n/static/0df7fd6cb7a86333f53b0b10f85f6633/4af54/attention-4.png 1400w,\n/static/0df7fd6cb7a86333f53b0b10f85f6633/faabe/attention-4.png 1509w\"\n        sizes=\"(max-width: 700px) 100vw, 700px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<h2>Multi Head Attention</h2>\n<p>When the number of head is one and the query, key and value dimensions are same then same network becomes self attention. But when we use multiple heads then the same network (key, query and value) gets multiplied with the number of heads and produce those many feature maps. Let's take the same example and extend it for the multiple heads.\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 700px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/b929cb787bd7bd2e4cba485ca31f1a71/799c6/attention-5.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 160.57142857142856%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAgCAYAAAASYli2AAAACXBIWXMAABJ0AAASdAHeZh94AAAEGklEQVRIx61WaXPrNgwEL92HJeqW7NhOHOfotJn2//+27RC06yj1e3nT6QcMbZEEFwsQSyIi/M92+aEUZLGBsg1k3UBuKqi2h8xLyLKCiBPITQ1lW4goBgWh3xcEEFnOc6TNzaGIIog4hcwLv6Cs+AD3X5Yb/qZsB1lZiDRjByKM/N4wgrgecHXoULlNIskYAW9I85uFER9wneP1cfLjkN0kn2gMIyKtGYlIUg5PzzuopuNvTEVWfMNhEEDW1jt2i01wCdeHrrrhytEvJkUIz4ULQxtG5pCqpmdkPlE1c8iJclynOc/dd/jFOFzmrYS0rUfZDp7bJPWRuDHNQFLeLxs1zv7UfvKZdKgcokvYjgY9baH68Z9ScRHo3fEzHVfP4naKkN6U9uVgAv7tvjEVRckU8Zo1Ou+Qw3MhXIqX+YliXyJcdznPueyqYeYEuqw75G7k0F2Bh6F3yNBdXUUx9LT4jU3HmZWuTFyIznlRQg0TpKvXrAQlBd8o0QygZoLc2EvILpwrB0EEUgGEC9NZlILiAiQIZCJQVDA90ggEZQHSEcJAIK8yqLTwDnVkoNMURBpKE8JNDjIxQknomwi5zaGDBMepwLyUCIsa27bEYciQ2ArLMOL92GDTW1AWBThODaalgslK7NoNnuYCeVNh7Ce8P+7xsLWww4zn4yPO+x7z4YCXl3ecH2aMuwG7wwmnwxbbnQu9afB6fsXTtkO3zHh+fsFv5yPa3qIfF7ycn7HfDrBdh+2yYGwrVHWFeZoxthZFnqFtW9RlibLIQUmS8MKhs0jTFMMwoKtrFEUObTSiKIRxN4cEwjBm/sTnEvtqQggoIb69o0IQlPLjt3dZ5AXUuMBGAw404ViF2LcCTdwjjregzSuKacH7K8Ha2+ZaCXRKoJJi7dDVW/D7X+jzPT7ohDeb4jwR5nxBFi4g+4F8OeDPD0LX3RxOWuIcKAz6y12WVQ1zfkebP+APOuG0SfG2ELq0RRz0jDC0C06P/3Z4ChSPYuXQtjDPb6jjAU+0YFeGmCyhjCxiXYGyLVSSYxoJcXxzOGuJp0DhwUioFYcmhIwzGBEgpxSZlkgDgqEIgYhAMoNRIapCr5KyaIm3QGGrJeTX9iVJQQoNVdTQVQ/dzDDNANPNCOc9dN2D7LDKaK8Eo9x/Rbgqj0unZqW7NtJLExVar9Y2SrDT5R7ClUnJneWTNN41IwijlkjEL0iA6zQs6D9xmDiURFyLriZj8VWkWEdCRuZkkpuq64v9yD2SG7CTBCcNTiGTFGFRItIahgjByqHWvlmyPode/aRireF+6XrjlWenju5wR4lbJ+6IlFvESPrRC/1FT2Tdcp06GXAH8ovhqnY/1eVLMtY8Gv9Q4udIdjMnB3XjNfmeSP0X4/JyLy6zfk38DQ7OuScJu3SNAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"attention 5\"\n        title=\"attention 5\"\n        src=\"/static/b929cb787bd7bd2e4cba485ca31f1a71/39600/attention-5.png\"\n        srcset=\"/static/b929cb787bd7bd2e4cba485ca31f1a71/1aaec/attention-5.png 175w,\n/static/b929cb787bd7bd2e4cba485ca31f1a71/98287/attention-5.png 350w,\n/static/b929cb787bd7bd2e4cba485ca31f1a71/39600/attention-5.png 700w,\n/static/b929cb787bd7bd2e4cba485ca31f1a71/799c6/attention-5.png 904w\"\n        sizes=\"(max-width: 700px) 100vw, 700px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<h2>References</h2>\n<div class=\"footnotes\">\n<hr>\n<ol>\n<li id=\"fn-1\">Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... &#x26; Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30.<a href=\"#fnref-1\" class=\"footnote-backref\">↩</a></li>\n<li id=\"fn-2\"><a href=\"https://www.youtube.com/watch?v=mmzRYGCfTzc\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://www.youtube.com/watch?v=mmzRYGCfTzc</a><a href=\"#fnref-2\" class=\"footnote-backref\">↩</a></li>\n<li id=\"fn-3\"><a href=\"https://www.youtube.com/watch?v=WyuZvGt7RY4&#x26;t=2207s\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://www.youtube.com/watch?v=WyuZvGt7RY4&#x26;t=2207s</a><a href=\"#fnref-3\" class=\"footnote-backref\">↩</a></li>\n<li id=\"fn-4\"><a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/MultiHeadAttention\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://www.tensorflow.org/api_docs/python/tf/keras/layers/MultiHeadAttention</a><a href=\"#fnref-4\" class=\"footnote-backref\">↩</a></li>\n</ol>\n</div>"}},{"node":{"frontmatter":{"title":"1x1 Convolution","description":"Why does someone use a 1x1 convolution? And why is it called 1x1? It sounds like a pixel-sized joke! When it first showed up, people probably thought it was a prank. But hold on, let's dive deep into this mysterious little guy and unravel the math magic behind it.","slug":"/blog/OneXOne_Convolution","date":"2024-07-11","tags":["CNN","TensorFlow"],"draft":false},"html":"<h2>Overview</h2>\n<p>Recently, while diving into the Self-Attention Model applied to images, I came across the term <code class=\"language-text\">1x1 convolutions</code>. At first, it sounded amusing, but as I dug deeper to understand it, I was thoroughly impressed. To make sense of this concept, I crafted a three-step process: <font color=#64ffda>what is a 1x1 convolution</font>, <font color=#64ffda>how it works</font>, <font color=#64ffda>math behind this</font> and <font color=#64ffda>why it's useful</font>.</p>\n<h2>Some Background on Convolution Operation</h2>\n<p>To understand the 1x1 convolution, we first need to grasp the convolution operation and the complexity involved in the underlying process<sup id=\"fnref-1\"><a href=\"#fn-1\" class=\"footnote-ref\">1</a></sup>. Consider an RGB image with dimensions 6x6x3, and we want to apply a convolution layer with a filter size of 3x3, with 2 filters, and a ReLU activation function. Let's denote the output by 'O'. The mathematical representation for this operation is given below. Here, 'F' represents the filter, the superscript indicates the filter number, and the subscript denotes the input channel.\r\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 700px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/17fc3b7753c6d02aedf1d16a0336bcc2/7a411/CNN-1.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 55.42857142857143%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAALABQDASIAAhEBAxEB/8QAFwAAAwEAAAAAAAAAAAAAAAAAAAIDBf/EABYBAQEBAAAAAAAAAAAAAAAAAAEAAv/aAAwDAQACEAMQAAABw6QYgQ0f/8QAGBAAAgMAAAAAAAAAAAAAAAAAAAEQEUH/2gAIAQEAAQUCLG4w/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPwE//8QAFREBAQAAAAAAAAAAAAAAAAAAABH/2gAIAQIBAT8BV//EABQQAQAAAAAAAAAAAAAAAAAAACD/2gAIAQEABj8CX//EABkQAAIDAQAAAAAAAAAAAAAAAAABESExkf/aAAgBAQABPyGaiDKosFpL6Hp//9oADAMBAAIAAwAAABCEz//EABYRAQEBAAAAAAAAAAAAAAAAAAEAIf/aAAgBAwEBPxB0gL//xAAXEQEBAQEAAAAAAAAAAAAAAAABABEh/9oACAECAQE/EM6Sr//EABkQAQEBAQEBAAAAAAAAAAAAAAERACFBMf/aAAgBAQABPxC2VHkymfJwZ8dV077nDizrnS3/2Q=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"CNN 1\"\n        title=\"CNN 1\"\n        src=\"/static/17fc3b7753c6d02aedf1d16a0336bcc2/03346/CNN-1.jpg\"\n        srcset=\"/static/17fc3b7753c6d02aedf1d16a0336bcc2/71299/CNN-1.jpg 175w,\n/static/17fc3b7753c6d02aedf1d16a0336bcc2/1e9fe/CNN-1.jpg 350w,\n/static/17fc3b7753c6d02aedf1d16a0336bcc2/03346/CNN-1.jpg 700w,\n/static/17fc3b7753c6d02aedf1d16a0336bcc2/c3223/CNN-1.jpg 1050w,\n/static/17fc3b7753c6d02aedf1d16a0336bcc2/da6ee/CNN-1.jpg 1400w,\n/static/17fc3b7753c6d02aedf1d16a0336bcc2/7a411/CNN-1.jpg 2048w\"\n        sizes=\"(max-width: 700px) 100vw, 700px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span>\r\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 700px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/94bd70374297ec4315a83015f7690229/ea27d/CNN-2.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 70.28571428571428%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAOABQDASIAAhEBAxEB/8QAGAAAAgMAAAAAAAAAAAAAAAAAAAECAwX/xAAUAQEAAAAAAAAAAAAAAAAAAAAA/9oADAMBAAIQAxAAAAHDdchCD//EABcQAAMBAAAAAAAAAAAAAAAAAAABEBH/2gAIAQEAAQUCqNn/xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/AT//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/AT//xAAUEAEAAAAAAAAAAAAAAAAAAAAg/9oACAEBAAY/Al//xAAZEAADAQEBAAAAAAAAAAAAAAAAAREhMVH/2gAIAQEAAT8hmptLfCZwSzToiFh//9oADAMBAAIAAwAAABAzz//EABURAQEAAAAAAAAAAAAAAAAAABAR/9oACAEDAQE/EKf/xAAVEQEBAAAAAAAAAAAAAAAAAAAQEf/aAAgBAgEBPxCH/8QAGxABAAIDAQEAAAAAAAAAAAAAAQARIUFhoTH/2gAIAQEAAT8QEgBBN+DU1mex16jcM/E9gzRP/9k='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"CNN 2\"\n        title=\"CNN 2\"\n        src=\"/static/94bd70374297ec4315a83015f7690229/03346/CNN-2.jpg\"\n        srcset=\"/static/94bd70374297ec4315a83015f7690229/71299/CNN-2.jpg 175w,\n/static/94bd70374297ec4315a83015f7690229/1e9fe/CNN-2.jpg 350w,\n/static/94bd70374297ec4315a83015f7690229/03346/CNN-2.jpg 700w,\n/static/94bd70374297ec4315a83015f7690229/c3223/CNN-2.jpg 1050w,\n/static/94bd70374297ec4315a83015f7690229/da6ee/CNN-2.jpg 1400w,\n/static/94bd70374297ec4315a83015f7690229/ea27d/CNN-2.jpg 1580w\"\n        sizes=\"(max-width: 700px) 100vw, 700px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span>\r\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 700px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/5a649a3ee1333e21e734009e50e48ac4/755c4/CNN-3.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 135.42857142857144%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAbABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAQACBf/EABUBAQEAAAAAAAAAAAAAAAAAAAAB/9oADAMBAAIQAxAAAAHhImLUYUUqT//EABYQAQEBAAAAAAAAAAAAAAAAAAEQIP/aAAgBAQABBQKNXf8A/8QAFBEBAAAAAAAAAAAAAAAAAAAAIP/aAAgBAwEBPwEf/8QAFBEBAAAAAAAAAAAAAAAAAAAAIP/aAAgBAgEBPwEf/8QAFBABAAAAAAAAAAAAAAAAAAAAMP/aAAgBAQAGPwJP/8QAHhAAAgEDBQAAAAAAAAAAAAAAAAERECAhMVFhcZH/2gAIAQEAAT8hSwR0JGwnCJ4Mmntrp//aAAwDAQACAAMAAAAQdw2w/8QAFhEAAwAAAAAAAAAAAAAAAAAAAREg/9oACAEDAQE/EEY//8QAFREBAQAAAAAAAAAAAAAAAAAAESD/2gAIAQIBAT8QI//EAB0QAQACAgMBAQAAAAAAAAAAAAEAIRExQVFxYZH/2gAIAQEAAT8QfkYx3BYxlPSxEWq4jh8O+PyFe3NlwHWwR3tPCO4NHkSBe4uW5//Z'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"CNN 3\"\n        title=\"CNN 3\"\n        src=\"/static/5a649a3ee1333e21e734009e50e48ac4/03346/CNN-3.jpg\"\n        srcset=\"/static/5a649a3ee1333e21e734009e50e48ac4/71299/CNN-3.jpg 175w,\n/static/5a649a3ee1333e21e734009e50e48ac4/1e9fe/CNN-3.jpg 350w,\n/static/5a649a3ee1333e21e734009e50e48ac4/03346/CNN-3.jpg 700w,\n/static/5a649a3ee1333e21e734009e50e48ac4/c3223/CNN-3.jpg 1050w,\n/static/5a649a3ee1333e21e734009e50e48ac4/755c4/CNN-3.jpg 1167w\"\n        sizes=\"(max-width: 700px) 100vw, 700px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span>\r\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 700px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/c449ab3257e394888aecf0bfcc9d19ef/f7a52/CNN-4.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 101.14285714285714%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAUABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAECBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAeHcUyozQgP/xAAYEAACAwAAAAAAAAAAAAAAAAAAARARIP/aAAgBAQABBQKGIvP/xAAUEQEAAAAAAAAAAAAAAAAAAAAg/9oACAEDAQE/AR//xAAUEQEAAAAAAAAAAAAAAAAAAAAg/9oACAECAQE/AR//xAAUEAEAAAAAAAAAAAAAAAAAAAAw/9oACAEBAAY/Ah//xAAZEAADAQEBAAAAAAAAAAAAAAAAAREQQSH/2gAIAQEAAT8hvgmxIxoUN18zmf/aAAwDAQACAAMAAAAQFBjA/8QAFxEAAwEAAAAAAAAAAAAAAAAAAAEREP/aAAgBAwEBPxCMgs//xAAVEQEBAAAAAAAAAAAAAAAAAAARIP/aAAgBAgEBPxAj/8QAHBABAAICAwEAAAAAAAAAAAAAAQARITFBUeGx/9oACAEBAAE/ECw+X7NirrJb7EocRMyu5yARrlDBTUWFuf/Z'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"CNN 4\"\n        title=\"CNN 4\"\n        src=\"/static/c449ab3257e394888aecf0bfcc9d19ef/03346/CNN-4.jpg\"\n        srcset=\"/static/c449ab3257e394888aecf0bfcc9d19ef/71299/CNN-4.jpg 175w,\n/static/c449ab3257e394888aecf0bfcc9d19ef/1e9fe/CNN-4.jpg 350w,\n/static/c449ab3257e394888aecf0bfcc9d19ef/03346/CNN-4.jpg 700w,\n/static/c449ab3257e394888aecf0bfcc9d19ef/c3223/CNN-4.jpg 1050w,\n/static/c449ab3257e394888aecf0bfcc9d19ef/da6ee/CNN-4.jpg 1400w,\n/static/c449ab3257e394888aecf0bfcc9d19ef/f7a52/CNN-4.jpg 1477w\"\n        sizes=\"(max-width: 700px) 100vw, 700px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span>\r\nLet's validate these numbers such as output feature map dimension and the number of learning parameter by implementing the same example in tensorflow.</p>\n<div class=\"gatsby-code-title\">Conv2D.py</div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> tensorflow <span class=\"token keyword\">as</span> tf\r\n<span class=\"token keyword\">from</span> tensorflow<span class=\"token punctuation\">.</span>keras<span class=\"token punctuation\">.</span>models <span class=\"token keyword\">import</span> Sequential\r\n<span class=\"token keyword\">from</span> tensorflow<span class=\"token punctuation\">.</span>keras<span class=\"token punctuation\">.</span>layers <span class=\"token keyword\">import</span> Conv2D\r\n\r\n<span class=\"token comment\"># Define the model</span>\r\nmodel <span class=\"token operator\">=</span> Sequential<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\r\n\r\n<span class=\"token comment\"># Add a 2D convolution layer</span>\r\nmodel<span class=\"token punctuation\">.</span>add<span class=\"token punctuation\">(</span>Conv2D<span class=\"token punctuation\">(</span>filters<span class=\"token operator\">=</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span> kernel_size<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span> <span class=\"token number\">3</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> activation<span class=\"token operator\">=</span><span class=\"token string\">'relu'</span><span class=\"token punctuation\">,</span> input_shape<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">6</span><span class=\"token punctuation\">,</span> <span class=\"token number\">6</span><span class=\"token punctuation\">,</span> <span class=\"token number\">3</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\r\n\r\n<span class=\"token comment\"># Summary of the model</span>\r\nmodel<span class=\"token punctuation\">.</span>summary<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>The output of the network is displayed below.\r\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 700px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/93a3a0c8a10ceae2ff6db5c235956606/a14f6/CNN-5.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 22.285714285714285%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAECAYAAACOXx+WAAAACXBIWXMAABJ0AAASdAHeZh94AAAApElEQVQY05WOuQ6EMBBDU0MFiEMhCBBHEm4F/v/XvPJIbLXNFpaTmRc7ap5nTNOEZVnAc57nKMsSRVGItNZY11UYet/3X4ZujMG2bRiGQe7Ke4/rumQYQsC+76jrGk3TCMyA53lAjoUsZjgZijNrrbBVVUF1XYe2bTGOoziD2JQkCdI0RZZlP8Xdu3+dbxQb+cP7vnGeJ47jgHNO2uI4RhRFf+kDoHtpvp02kaMAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"CNN 5\"\n        title=\"CNN 5\"\n        src=\"/static/93a3a0c8a10ceae2ff6db5c235956606/39600/CNN-5.png\"\n        srcset=\"/static/93a3a0c8a10ceae2ff6db5c235956606/1aaec/CNN-5.png 175w,\n/static/93a3a0c8a10ceae2ff6db5c235956606/98287/CNN-5.png 350w,\n/static/93a3a0c8a10ceae2ff6db5c235956606/39600/CNN-5.png 700w,\n/static/93a3a0c8a10ceae2ff6db5c235956606/a14f6/CNN-5.png 797w\"\n        sizes=\"(max-width: 700px) 100vw, 700px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<h2>What is 1x1 convolution</h2>\n<p>This concept was proposed by Min Lin et al. in 2013 in the paper <strong>Network in Network</strong><sup id=\"fnref-2\"><a href=\"#fn-2\" class=\"footnote-ref\">2</a></sup>. The idea was to use an MLP (Multi-Layer Perceptron) layer to introduce non-linearity into the network. The figure below illustrates the difference between a standard convolution layer and an MLP layer. The 1x1 convolution goes by various names, including Network in Network, MLP Conv, and Bottleneck layer in Inception<sup id=\"fnref-3\"><a href=\"#fn-3\" class=\"footnote-ref\">3</a></sup>.\r\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 700px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 46.857142857142854%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAABJ0AAASdAHeZh94AAABfUlEQVQoz22QS0/bQBSFR+rP6LZ/Fol1N+VZIQqLIsqCKqxQt10WkUBIUEhKcLCd+DGZsceer5oxEFO40tHccx/n3rnCWouz9966rt+N/x9r14l2whjDfD73r0Oapm9EpJQkSeJFnFVVRZZlL1y0p7lEHMdEUeShtX61SVEUPh8EAePxmDAMPVdKrTZ8dtwkN91t5TZwgk6gvZ3jz7nhcMhoNGI2m70V9PeqDHmWYEoNtkLmKXVdYq3B1sZdCqVykmTueVko1DInSxcoJX2PtXVzQ/8tIJWwLEFqiBaGouKV1U8oK8gUaNPUt02UpvICRdIl7O8R9g8I+994vN4nGhyST36g7o+Rk+PGn54QDw4Ienu+Lrs7Qk5OkJPvmPwWkd6fc7H/keHhBy6/CPrbgpsdwfW24GpL0N1oYr2Nhl9tCm6/Cm52BQOHHeH7/nwWLLrriCL+zd/TTyS9NfTjL5bTn+iHDstpBx10UA8NimDFne/yq/gZanqKkXf8A9HnmCXYWx+tAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"CNN 6\"\n        title=\"CNN 6\"\n        src=\"/static/3cd6454bfcc51716e1ad73281425bc50/39600/CNN-6.png\"\n        srcset=\"/static/3cd6454bfcc51716e1ad73281425bc50/1aaec/CNN-6.png 175w,\n/static/3cd6454bfcc51716e1ad73281425bc50/98287/CNN-6.png 350w,\n/static/3cd6454bfcc51716e1ad73281425bc50/39600/CNN-6.png 700w,\n/static/3cd6454bfcc51716e1ad73281425bc50/9d8f4/CNN-6.png 973w\"\n        sizes=\"(max-width: 700px) 100vw, 700px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span> Source: Page 2, Netowrk in Network (<a href=\"https://arxiv.org/pdf/1312.4400\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://arxiv.org/pdf/1312.4400</a>)</p>\n<p>Let's take the same example that we have taken intially in our discussion, but keep the filter size to (1x1) and now compute the feature map, number of learning parameters to understand this better.\r\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 700px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/97a2b9659cc77e96a52e73da2cf06def/821ce/CNN-7.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 113.7142857142857%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAXABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAECBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAeFmjKigyD//xAAZEAABBQAAAAAAAAAAAAAAAAABEBEgMUH/2gAIAQEAAQUCFajR/8QAFBEBAAAAAAAAAAAAAAAAAAAAIP/aAAgBAwEBPwEf/8QAFBEBAAAAAAAAAAAAAAAAAAAAIP/aAAgBAgEBPwEf/8QAFBABAAAAAAAAAAAAAAAAAAAAMP/aAAgBAQAGPwIf/8QAGhAAAgMBAQAAAAAAAAAAAAAAAAEQESFBUf/aAAgBAQABPyEHUEoYLesteuP/2gAMAwEAAgADAAAAENPPAP/EABQRAQAAAAAAAAAAAAAAAAAAACD/2gAIAQMBAT8QH//EABQRAQAAAAAAAAAAAAAAAAAAACD/2gAIAQIBAT8QH//EAB4QAAIBBAMBAAAAAAAAAAAAAAERACExQWEQUXHB/9oACAEBAAE/EBHqZBpqFOlobJIHcT66Qh7mISQNwqeBXPH/2Q=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"CNN 7\"\n        title=\"CNN 7\"\n        src=\"/static/97a2b9659cc77e96a52e73da2cf06def/03346/CNN-7.jpg\"\n        srcset=\"/static/97a2b9659cc77e96a52e73da2cf06def/71299/CNN-7.jpg 175w,\n/static/97a2b9659cc77e96a52e73da2cf06def/1e9fe/CNN-7.jpg 350w,\n/static/97a2b9659cc77e96a52e73da2cf06def/03346/CNN-7.jpg 700w,\n/static/97a2b9659cc77e96a52e73da2cf06def/c3223/CNN-7.jpg 1050w,\n/static/97a2b9659cc77e96a52e73da2cf06def/821ce/CNN-7.jpg 1371w\"\n        sizes=\"(max-width: 700px) 100vw, 700px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span>\r\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 700px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/da0304b48d7641b5a5722dda52a3afe2/63206/CNN-8.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 145.7142857142857%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAdABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAECBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAeDYMqNMgg//xAAVEAEBAAAAAAAAAAAAAAAAAAARMP/aAAgBAQABBQJp/8QAFBEBAAAAAAAAAAAAAAAAAAAAIP/aAAgBAwEBPwEf/8QAFBEBAAAAAAAAAAAAAAAAAAAAIP/aAAgBAgEBPwEf/8QAFBABAAAAAAAAAAAAAAAAAAAAMP/aAAgBAQAGPwJP/8QAGhAAAwEAAwAAAAAAAAAAAAAAAAEREDFBUf/aAAgBAQABPyFC+vKyjOij5y5//9oADAMBAAIAAwAAABA/C8z/xAAVEQEBAAAAAAAAAAAAAAAAAAAQEf/aAAgBAwEBPxCEP//EABURAQEAAAAAAAAAAAAAAAAAABAR/9oACAECAQE/EIQ//8QAHBABAQEAAwEBAQAAAAAAAAAAAREAITFBcYGR/9oACAEBAAE/EIDa+cMzw5ImZeMB6zUnNcq1G4Tu+fNM7f5naq/cdfmZUmUXrf/Z'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"CNN 8\"\n        title=\"CNN 8\"\n        src=\"/static/da0304b48d7641b5a5722dda52a3afe2/03346/CNN-8.jpg\"\n        srcset=\"/static/da0304b48d7641b5a5722dda52a3afe2/71299/CNN-8.jpg 175w,\n/static/da0304b48d7641b5a5722dda52a3afe2/1e9fe/CNN-8.jpg 350w,\n/static/da0304b48d7641b5a5722dda52a3afe2/03346/CNN-8.jpg 700w,\n/static/da0304b48d7641b5a5722dda52a3afe2/c3223/CNN-8.jpg 1050w,\n/static/da0304b48d7641b5a5722dda52a3afe2/63206/CNN-8.jpg 1108w\"\n        sizes=\"(max-width: 700px) 100vw, 700px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>Let's examine the learning parameters for the first and second examples. For the first example, the calculation is <code class=\"language-text\">(1x1x3+1)*2</code>, because the filter size is 1x1 with 3 channels, resulting in a total of 8 learning parameters. In contrast, with a 3x3 convolution, the number of learning parameters is 56. This demonstrates how 1x1 convolutions help reduce computational complexity. Now, let's highlight some of the advantages of using 1x1 convolutions.</p>\n<h2>Advantages of 1x1 Convolution</h2>\n<blockquote>\n<ol>\n<li>Reduces the number of channels</li>\n</ol>\n</blockquote>\n<p>Using 1x1 convolutions helps reduce the number of channels without affecting the feature map. If we want to reduce the spatial dimensions of the feature map, a pooling layer would suffice. However, to reduce the number of channels, 1x1 convolutions are the way to go. A great example of this complexity reduction is explained by Andrew Yan-Tak Ng in the referenced video<sup id=\"fnref-4\"><a href=\"#fn-4\" class=\"footnote-ref\">4</a></sup></p>\n<blockquote>\n<ol start=\"2\">\n<li>Reduces Complexity</li>\n</ol>\n</blockquote>\n<p>As seen in the above example, the number of trainable parameters for a 3x3 convolution is 56, while for a 1x1 convolution, it is 8. This demonstrates how significantly 1x1 convolutions reduce the complexity of the neural network.</p>\n<blockquote>\n<ol start=\"3\">\n<li>Adding non-linarity</li>\n</ol>\n</blockquote>\n<p>We know that non-linearity introduced through activation functions enhances learning and aids in latent feature representation. The 1x1 convolution operation helps increase this non-linearity. For example, in the case of 3x3 and 1x1 convolutions, the non-linear functions are applied 16 and 36 times, respectively. This demonstrates how 1x1 convolutions can be advantageous in such scenarios.</p>\n<h2>References</h2>\n<div class=\"footnotes\">\n<hr>\n<ol>\n<li id=\"fn-1\">A good tutorial on CNN, <a href=\"https://cs231n.github.io/convolutional-networks\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://cs231n.github.io/convolutional-networks</a><a href=\"#fnref-1\" class=\"footnote-backref\">↩</a></li>\n<li id=\"fn-2\">Lin, M., Chen, Q., &#x26; Yan, S. (2013). Network in network. arXiv preprint arXiv:1312.4400.<a href=\"#fnref-2\" class=\"footnote-backref\">↩</a></li>\n<li id=\"fn-3\">Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., ... &#x26; Rabinovich, A. (2015). Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 1-9).<a href=\"#fnref-3\" class=\"footnote-backref\">↩</a></li>\n<li id=\"fn-4\">1x1 in Inception Netoerk, <a href=\"https://youtu.be/C86ZXvgpejM?si=z-farl2kDcZcRxEv\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://youtu.be/C86ZXvgpejM?si=z-farl2kDcZcRxEv</a><a href=\"#fnref-4\" class=\"footnote-backref\">↩</a></li>\n</ol>\n</div>"}}]}},"pageContext":{}},"staticQueryHashes":["3115057458"]}