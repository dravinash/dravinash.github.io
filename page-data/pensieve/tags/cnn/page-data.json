{"componentChunkName":"component---src-templates-tag-js","path":"/pensieve/tags/cnn/","result":{"data":{"allMarkdownRemark":{"totalCount":2,"edges":[{"node":{"frontmatter":{"title":"Attention Please!","description":"Attention in machine learning is like the brain's spotlight, highlighting important parts of data. Introduced in 2014, it focuses on relevant information using \"queries,\" \"keys,\" and \"values.\" Imagine a model yelling, \"Hey, brain, remember that detail!\" to handle complex tasks like translating long sentences.","date":"2024-07-22","slug":"/blog_posts/attention","tags":["CNN","Attention","TensorFlow"]}}},{"node":{"frontmatter":{"title":"1x1 Convolution","description":"Why does someone use a 1x1 convolution? And why is it called 1x1? It sounds like a pixel-sized joke! When it first showed up, people probably thought it was a prank. But hold on, let's dive deep into this mysterious little guy and unravel the math magic behind it.","date":"2024-07-11","slug":"/blog/OneXOne_Convolution","tags":["CNN","TensorFlow"]}}}]}},"pageContext":{"tag":"CNN"}},"staticQueryHashes":["3115057458"]}